{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab507c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nabeel Ali\\AppData\\Roaming\\Python\\Python39\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Nabeel\n",
      "[nltk_data]     Ali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Nabeel\n",
      "[nltk_data]     Ali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Nabeel\n",
      "[nltk_data]     Ali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "# Load dataset\n",
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "# Check the first few rows and column names\n",
    "# print(df.head())\n",
    "# print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed23e0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (per label):\n",
      "anger: 0.8700\n",
      "fear: 0.6462\n",
      "joy: 0.7960\n",
      "sadness: 0.7058\n",
      "surprise: 0.6895\n",
      "\n",
      "Classification Report (overall):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.00      0.00      0.00        72\n",
      "        fear       0.64      0.92      0.76       330\n",
      "         joy       0.75      0.03      0.05       115\n",
      "     sadness       0.61      0.07      0.12       167\n",
      "    surprise       0.82      0.05      0.09       179\n",
      "\n",
      "   micro avg       0.65      0.38      0.48       863\n",
      "   macro avg       0.56      0.21      0.20       863\n",
      "weighted avg       0.63      0.38      0.34       863\n",
      " samples avg       0.55      0.35      0.41       863\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nabeel Ali\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Nabeel Ali\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Nabeel Ali\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# import nltk\n",
    "# import re\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.stem.porter import PorterStemmer\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# nltk.download('punkt_tab')\n",
    "# paragraph = \"\"\"This involved swimming a pretty large lake that was over my head. \n",
    "# It was one of my most shameful experiences. \n",
    "# After all, I had vegetables coming out my ears all for the benefit of the young prince.\"\"\"\n",
    "\n",
    "# ps = PorterStemmer()\n",
    "# wordnet = WordNetLemmatizer()\n",
    "# sentences = nltk.sent_tokenize(paragraph)\n",
    "# corpus = []\n",
    "\n",
    "# for i in range(len(sentences)):\n",
    "#     review = re.sub('[^a-zA-Z]',' ' ,sentences[i])\n",
    "#     review = review.lower()\n",
    "#     review = review.split()\n",
    "#     review = [wordnet.lemmatize(word) for word in review if not word in set (stopwords.words('english'))]\n",
    "#     review = ' '.join(review)\n",
    "#     corpus.append(review)\n",
    "\n",
    "# print(corpus)\n",
    "# # ['involv swim pretti larg lake head', 'one shame experi', 'veget come ear benefit young princ']\n",
    "\n",
    "# # from sklearn.feature_extraction.text import CountVectorizer\n",
    "# # cv = CountVectorizer()\n",
    "# # x = cv.fit_transform(corpus).toarray()\n",
    "\n",
    "# # print(x)\n",
    "\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# cv1 = TfidfVectorizer()\n",
    "# y = cv1.fit_transform(corpus).toarray()\n",
    "\n",
    "\n",
    "# print(y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Extract text and emotion columns\n",
    "texts = df['text']  # assuming your text column is named 'text'\n",
    "emotions = df[['anger', 'fear', 'joy', 'sadness', 'surprise']]  # multi-label target\n",
    "\n",
    "# Preprocessing\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "corpus = []\n",
    "\n",
    "for sentence in texts:\n",
    "    review = re.sub('[^a-zA-Z]', ' ', str(sentence))\n",
    "    review = review.lower().split()\n",
    "    review = [lemmatizer.lemmatize(word) for word in review if word not in stop_words]\n",
    "    corpus.append(' '.join(review))\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus).toarray()\n",
    "y = emotions.values  # multi-label targets\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Multinomial Naive Bayes with multi-label wrapper\n",
    "nb = MultinomialNB()\n",
    "multi_target_nb = MultiOutputClassifier(nb)\n",
    "multi_target_nb.fit(X_train, y_train)\n",
    "\n",
    "# print(X_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = multi_target_nb.predict(X_test)\n",
    "\n",
    "\n",
    "# Evaluate\n",
    "print(\"Accuracy (per label):\")\n",
    "for i, emotion in enumerate(emotions.columns):\n",
    "    print(f\"{emotion}: {accuracy_score(y_test[:, i], y_pred[:, i]):.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report (overall):\")\n",
    "print(classification_report(y_test, y_pred, target_names=emotions.columns))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e93a6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted emotions:\n",
      "anger: No\n",
      "fear: Yes\n",
      "joy: No\n",
      "sadness: Yes\n",
      "surprise: No\n"
     ]
    }
   ],
   "source": [
    "new_text = \"shameful experience\"\n",
    "\n",
    "# Preprocess\n",
    "review = re.sub('[^a-zA-Z]', ' ', new_text)\n",
    "review = review.lower().split()\n",
    "review = [lemmatizer.lemmatize(word) for word in review if word not in stop_words]\n",
    "processed_text = ' '.join(review)\n",
    "\n",
    "# Vectorize using the same TF-IDF vectorizer\n",
    "X_new = vectorizer.transform([processed_text]).toarray()\n",
    "\n",
    "# Predict\n",
    "predicted_labels = multi_target_nb.predict(X_new)[0]\n",
    "\n",
    "# Print result\n",
    "print(\"\\nPredicted emotions:\")\n",
    "for emotion, value in zip(emotions.columns, predicted_labels):\n",
    "    print(f\"{emotion}: {'Yes' if value == 1 else 'No'}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "# import math\n",
    "# from collections import defaultdict, Counter\n",
    "\n",
    "# class NaiveBayesClassifier:\n",
    "#     def __init__(self):\n",
    "#         self.class_priors = {}\n",
    "#         self.word_counts = {}\n",
    "#         self.class_word_totals = {}\n",
    "#         self.vocabulary = set()\n",
    "#         self.classes = set()\n",
    "\n",
    "#     def tokenize(self, text):\n",
    "#         return text.lower().split()\n",
    "\n",
    "#     def train(self, csv_file):\n",
    "#         df = pd.read_csv(csv_file)\n",
    "#         class_counts = Counter()\n",
    "#         word_counts = defaultdict(Counter)\n",
    "\n",
    "#         for _, row in df.iterrows():\n",
    "#             label = row['class']\n",
    "#             tokens = self.tokenize(row['sentence'])\n",
    "#             class_counts[label] += 1\n",
    "#             word_counts[label].update(tokens)\n",
    "#             self.vocabulary.update(tokens)\n",
    "#             self.classes.add(label)\n",
    "\n",
    "#         self.class_priors = {cls: count / sum(class_counts.values()) for cls, count in class_counts.items()}\n",
    "#         self.word_counts = word_counts\n",
    "#         self.class_word_totals = {cls: sum(word_counts[cls].values()) for cls in self.classes}\n",
    "#         self.vocabulary.add('<UNK>')  # unknown word token\n",
    "\n",
    "#     def predict(self, sentence):\n",
    "#         tokens = self.tokenize(sentence)\n",
    "#         log_probs = {}\n",
    "\n",
    "#         for cls in self.classes:\n",
    "#             log_prob = math.log(self.class_priors[cls])\n",
    "#             total_words = self.class_word_totals[cls]\n",
    "#             vocab_size = len(self.vocabulary)\n",
    "\n",
    "#             for token in tokens:\n",
    "#                 count = self.word_counts[cls].get(token, 0)\n",
    "#                 prob = (count + 1) / (total_words + vocab_size)  # add-1 smoothing\n",
    "#                 log_prob += math.log(prob)\n",
    "\n",
    "#             log_probs[cls] = log_prob\n",
    "\n",
    "#         return max(log_probs, key=log_probs.get)\n",
    "\n",
    "# # Example usage:\n",
    "# # Create and train classifier\n",
    "# nb = NaiveBayesClassifier()\n",
    "# nb.train('sample_sentences.csv')  # CSV should have columns: 'sentence', 'class'\n",
    "\n",
    "# # Predict new sentence\n",
    "# print(nb.predict(\"Dead\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4600a146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel, BertPreTrainedModel\n",
    "from torch.optim import AdamW\n",
    "from transformers import BertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('dataset.csv')\n",
    "texts = df['text'].tolist()\n",
    "labels = df[['anger', 'fear', 'joy', 'sadness', 'surprise']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51241b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForMultiLabel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.3369\n",
      "Epoch 2 Loss: 0.2695\n",
      "Epoch 3 Loss: 0.2345\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.83      0.28      0.42        72\n",
      "        fear       0.81      0.80      0.81       330\n",
      "         joy       0.62      0.73      0.67       115\n",
      "     sadness       0.77      0.54      0.63       167\n",
      "    surprise       0.80      0.70      0.75       179\n",
      "\n",
      "   micro avg       0.77      0.68      0.72       863\n",
      "   macro avg       0.77      0.61      0.65       863\n",
      "weighted avg       0.78      0.68      0.71       863\n",
      " samples avg       0.66      0.63      0.62       863\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nabeel Ali\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Nabeel Ali\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Train/test split\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.FloatTensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "train_dataset = EmotionDataset(train_texts, train_labels, tokenizer)\n",
    "test_dataset = EmotionDataset(test_texts, test_labels, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# Define BERT model for multi-label classification\n",
    "class BertForMultiLabel(BertPreTrainedModel):\n",
    "    def __init__(self, config, num_labels=5):\n",
    "        super().__init__(config)\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.init_weights()\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        pooled_output = self.dropout(outputs.pooler_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        probs = self.sigmoid(logits)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.BCELoss()\n",
    "            loss = loss_fn(probs, labels)\n",
    "        return {'loss': loss, 'logits': probs}\n",
    "\n",
    "from transformers import BertConfig\n",
    "config = BertConfig.from_pretrained('bert-base-uncased')\n",
    "model = BertForMultiLabel.from_pretrained('bert-base-uncased', config=config)\n",
    "\n",
    "# Training setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs['loss']\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1} Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].cpu().numpy()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        probs = outputs['logits'].cpu().numpy()\n",
    "        preds = (probs >= 0.5).astype(int)\n",
    "        all_preds.append(preds)\n",
    "        all_true.append(labels)\n",
    "\n",
    "y_pred = np.vstack(all_preds)\n",
    "y_true = np.vstack(all_true)\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=['anger', 'fear', 'joy', 'sadness', 'surprise']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0dc9bff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted emotions: ['fear']\n"
     ]
    }
   ],
   "source": [
    "# New input\n",
    "new_text = \"Oh no I need to get to the meeting very quickly or else manager will shout at me\"\n",
    "\n",
    "# Tokenize\n",
    "encoding = tokenizer(\n",
    "    new_text,\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    max_length=128,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Move to device\n",
    "input_ids = encoding['input_ids'].to(device)\n",
    "attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "# Model prediction\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    probs = outputs['logits'].cpu().numpy()[0]\n",
    "    preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "# Print predicted emotions\n",
    "emotion_labels = ['anger', 'fear', 'joy', 'sadness', 'surprise']\n",
    "predicted_emotions = [emotion_labels[i] for i, val in enumerate(preds) if val == 1]\n",
    "\n",
    "print(\"Predicted emotions:\", predicted_emotions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
